---------------
1. Load data
---------------

A. Define schema
----------------

scala> val id = StructField("account_id", IntegerType)
id: org.apache.spark.sql.types.StructField = StructField(account_id,IntegerType,true)

scala> val total_matches = StructField("total_matches", IntegerType)
total_matches: org.apache.spark.sql.types.StructField = StructField(total_matches,IntegerType,true)

scala> val total_wins = StructField("total_wins", IntegerType)
total_wins: org.apache.spark.sql.types.StructField = StructField(total_wins,IntegerType,true)

scala> val trueskill_mu = StructField("trueskill_mu", DoubleType)
trueskill_mu: org.apache.spark.sql.types.StructField = StructField(trueskill_mu,DoubleType,true)

scala> val trueskill_sigma = StructField("trueskill_sigma", DoubleType)
trueskill_sigma: org.apache.spark.sql.types.StructField = StructField(trueskill_sigma,DoubleType,true)

scala> val fields = Array(id, total_matches, total_wins, trueskill_mu, trueskill_sigma)
fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(account_id,IntegerType,true), StructField(total_matches,IntegerType,true), StructField(total_wins,IntegerType,true), StructField(trueskill_mu,DoubleType,true), StructField(trueskill_sigma,DoubleType,true))

scala> val schema = StructType(fields)
schema: org.apache.spark.sql.types.StructType = StructType(StructField(account_id,IntegerType,true), StructField(total_matches,IntegerType,true), StructField(total_wins,IntegerType,true), StructField(trueskill_mu,DoubleType,true), StructField(trueskill_sigma,DoubleType,true))


B. Load Dota2 player rating data.
-----------------------------------

scala> val df1 = spark.read.schema(schema).option("header", true).csv("hdfs://localhost:9000/user/mingyuan/pr/player_ratings.csv")
df1: org.apache.spark.sql.DataFrame = [account_id: int, total_matches: int ... 3 more fields]

C. Show data
------------------

scala> df1.show
+----------+-------------+----------+------------------+------------------+
|account_id|total_matches|total_wins|      trueskill_mu|   trueskill_sigma|
+----------+-------------+----------+------------------+------------------+
|    236579|           14|        24| 27.86803544887669| 5.212360755154124|
|      -343|            1|         1| 26.54416264494717|  8.06547547571326|
|     -1217|            1|         1|26.521102844769825| 8.114989027066821|
|     -1227|            1|         1|27.248024958648585| 8.092216692198111|
|     -1284|            0|         1|22.931015779479623| 8.092224309040315|
|    308663|            1|         1|26.761475988284694| 8.108879563800533|
|     79749|           21|        40| 30.55341692111649|3.8687344175280614|
|     -1985|            0|         1|23.263409234993325| 8.098020435825411|
|     -2160|            8|        12|  27.4260179194793| 6.391300186726303|
|     26500|           26|        50| 27.94362077676565|4.0490045303341855|
|     -2776|            0|         1|23.053521554831327|   8.1109113767047|
|    137046|           46|        89| 26.02599787812959| 2.865184074214484|
|     56881|           15|        23| 32.85642396786685| 5.132468772471912|
|     -3110|            1|         2|25.491041376532355| 7.846784862628624|
|     -3126|            0|         1| 23.28169402641756|  8.11925868487977|
|    221202|            6|        12| 22.75076350501192| 6.198528539862464|
|     -3161|            0|         3|20.483374255793766| 7.757888786330731|
|     -3194|            0|         1| 22.89152055167358| 8.100930344663169|
|    221204|            3|         8|21.293206583904983| 6.560565350911063|
|     -3421|            2|         4| 23.66957648440773| 7.481699509258572|
+----------+-------------+----------+------------------+------------------+
only showing top 20 rows

D. Print schema
--------------------

scala> df1.printSchema()
root
 |-- account_id: integer (nullable = true)
 |-- total_matches: integer (nullable = true)
 |-- total_wins: integer (nullable = true)
 |-- trueskill_mu: double (nullable = true)
 |-- trueskill_sigma: double (nullable = true)

--------------------
2. DataFrame count
--------------------

scala> df1.count()
res19: Long = 834226 


----------------
3. Spark SQL
----------------

scala> df1.createOrReplaceTempView("dota2Player")

cala> val dedicatedPlayerDF = spark.sql("SELECT account_id,total_matches FROM dota2Player WHERE total_matches > 100 ORDER BY total_matches DESC")
dedicatedPlayerDF: org.apache.spark.sql.DataFrame = [account_id: int, total_matches: int]

scala> dedicatedPlayerDF.show()
+----------+-------------+                                                      
|account_id|total_matches|
+----------+-------------+
|         0|      1608398|
|      6647|          396|
|     12357|          341|
|     77866|          302|
|     35770|          290|
|      8101|          274|
|     17194|          271|
|     55446|          269|
|     28485|          262|
|     24722|          255|
|      2765|          251|
|    138306|          247|
|     52594|          245|
|     33769|          243|
|      3627|          241|
|     10069|          240|
|    116900|          240|
|     96383|          237|
|    297414|          237|
|    282413|          235|
+----------+-------------+
only showing top 20 rows


--------------
4. Simple LR
--------------

A. Features assembler
---------------------------

scala> import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors

scala> val assembler = new VectorAssembler().setInputCols(Array("total_matches", "total_wins")).setOutputCol("features")
assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_b78134c8668f

scala> val featuresDF = assembler.transform(df1)
featuresDF: org.apache.spark.sql.DataFrame = [account_id: int, total_matches: int ... 4 more fields]

scala> featuresDF.show
+----------+-------------+----------+------------------+------------------+-----------+
|account_id|total_matches|total_wins|      trueskill_mu|   trueskill_sigma|   features|
+----------+-------------+----------+------------------+------------------+-----------+
|    236579|           14|        24| 27.86803544887669| 5.212360755154124|[14.0,24.0]|
|      -343|            1|         1| 26.54416264494717|  8.06547547571326|  [1.0,1.0]|
|     -1217|            1|         1|26.521102844769825| 8.114989027066821|  [1.0,1.0]|
|     -1227|            1|         1|27.248024958648585| 8.092216692198111|  [1.0,1.0]|
|     -1284|            0|         1|22.931015779479623| 8.092224309040315|  [0.0,1.0]|
|    308663|            1|         1|26.761475988284694| 8.108879563800533|  [1.0,1.0]|
|     79749|           21|        40| 30.55341692111649|3.8687344175280614|[21.0,40.0]|
|     -1985|            0|         1|23.263409234993325| 8.098020435825411|  [0.0,1.0]|
|     -2160|            8|        12|  27.4260179194793| 6.391300186726303| [8.0,12.0]|
|     26500|           26|        50| 27.94362077676565|4.0490045303341855|[26.0,50.0]|
|     -2776|            0|         1|23.053521554831327|   8.1109113767047|  [0.0,1.0]|
|    137046|           46|        89| 26.02599787812959| 2.865184074214484|[46.0,89.0]|
|     56881|           15|        23| 32.85642396786685| 5.132468772471912|[15.0,23.0]|
|     -3110|            1|         2|25.491041376532355| 7.846784862628624|  [1.0,2.0]|
|     -3126|            0|         1| 23.28169402641756|  8.11925868487977|  [0.0,1.0]|
|    221202|            6|        12| 22.75076350501192| 6.198528539862464| [6.0,12.0]|
|     -3161|            0|         3|20.483374255793766| 7.757888786330731|  [0.0,3.0]|
|     -3194|            0|         1| 22.89152055167358| 8.100930344663169|  [0.0,1.0]|
|    221204|            3|         8|21.293206583904983| 6.560565350911063|  [3.0,8.0]|
|     -3421|            2|         4| 23.66957648440773| 7.481699509258572|  [2.0,4.0]|
+----------+-------------+----------+------------------+------------------+-----------+
only showing top 20 rows

B. Change column name
-------------------------------

scala> val newN = Seq("id", " "," ","label", "", "features")
newN: Seq[String] = List(id, " ", " ", label, "", features)

scala> val lrDF=featuresDF.toDF(newN:_*)
lrDF: org.apache.spark.sql.DataFrame = [id: int,  : int ... 4 more fields]

scala> lrDF.show
+------+---+---+------------------+------------------+-----------+
|    id|   |   |             label|                  |   features|
+------+---+---+------------------+------------------+-----------+
|236579| 14| 24| 27.86803544887669| 5.212360755154124|[14.0,24.0]|
|  -343|  1|  1| 26.54416264494717|  8.06547547571326|  [1.0,1.0]|
| -1217|  1|  1|26.521102844769825| 8.114989027066821|  [1.0,1.0]|
| -1227|  1|  1|27.248024958648585| 8.092216692198111|  [1.0,1.0]|
| -1284|  0|  1|22.931015779479623| 8.092224309040315|  [0.0,1.0]|
|308663|  1|  1|26.761475988284694| 8.108879563800533|  [1.0,1.0]|
| 79749| 21| 40| 30.55341692111649|3.8687344175280614|[21.0,40.0]|
| -1985|  0|  1|23.263409234993325| 8.098020435825411|  [0.0,1.0]|
| -2160|  8| 12|  27.4260179194793| 6.391300186726303| [8.0,12.0]|
| 26500| 26| 50| 27.94362077676565|4.0490045303341855|[26.0,50.0]|
| -2776|  0|  1|23.053521554831327|   8.1109113767047|  [0.0,1.0]|
|137046| 46| 89| 26.02599787812959| 2.865184074214484|[46.0,89.0]|
| 56881| 15| 23| 32.85642396786685| 5.132468772471912|[15.0,23.0]|
| -3110|  1|  2|25.491041376532355| 7.846784862628624|  [1.0,2.0]|
| -3126|  0|  1| 23.28169402641756|  8.11925868487977|  [0.0,1.0]|
|221202|  6| 12| 22.75076350501192| 6.198528539862464| [6.0,12.0]|
| -3161|  0|  3|20.483374255793766| 7.757888786330731|  [0.0,3.0]|
| -3194|  0|  1| 22.89152055167358| 8.100930344663169|  [0.0,1.0]|
|221204|  3|  8|21.293206583904983| 6.560565350911063|  [3.0,8.0]|
| -3421|  2|  4| 23.66957648440773| 7.481699509258572|  [2.0,4.0]|
+------+---+---+------------------+------------------+-----------+
only showing top 20 rows

C. Split data into traning data and validation data
---------------------------------------------------------

scala> val split = lrDF.randomSplit(Array(0.9,0.1))
split: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([id: int,  : int ... 4 more fields], [id: int,  : int ... 4 more fields])

scala> val train = split(0).cache()
train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int,  : int ... 4 more fields]

scala> val validation = split(1)
validation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int,  : int ... 4 more fields]


D. Train a simple linear regression model
-------------------------------------------------

scala> val model = lr.fit(train)
17/09/26 10:13:36 WARN optim.WeightedLeastSquares: regParam is zero, which might cause numerical instability and overfitting.
17/09/26 10:13:36 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/09/26 10:13:36 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/09/26 10:13:38 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
17/09/26 10:13:38 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
model: org.apache.spark.ml.regression.LinearRegressionModel = linReg_b23de0832693


E. Show results
---------------------

scala> model.transform(validation).show()
+----------+---+---+------------------+------------------+---------+------------------+
|        id|   |   |             label|                  | features|        prediction|
+----------+---+---+------------------+------------------+---------+------------------+
|-128396584|  1|  1|27.918812376134106| 8.053308860258714|[1.0,1.0]|25.636305865382393|
|-128396327|  2|  5|23.522165162753314| 7.258348927823103|[2.0,5.0]|24.238425636682162|
|-128391906|  1|  1|28.161518027242852| 8.019844081617187|[1.0,1.0]|25.636305865382393|
|-128385801|  3|  4|26.848128667469677| 7.370802079258343|[3.0,4.0]|26.445372362442924|
|-128384439|  0|  4|19.303048628447982| 7.534733942409549|[0.0,4.0]| 21.98742835783724|
|-128382256|  0|  2|22.554251369341607| 7.990255691229144|[0.0,2.0]|23.429359139621635|
|-128379426|  2|  2| 31.09536931318958|7.7954858068257025|[2.0,2.0]| 26.40132180935876|
|-128362765|  1|  3| 23.38138537665235| 7.533235212298229|[1.0,3.0]|24.194375083597997|
|-128357274|  0|  1| 23.54096127606779|  8.09166616855586|[0.0,1.0]|24.150324530513835|
|-128356857|  1|  2| 27.07600480063651|7.7138069904713555|[1.0,2.0]|24.915340474490197|
|-128356527|  1|  1| 26.14823703813555| 8.143672786544798|[1.0,1.0]|25.636305865382393|
|-128355496|  2|  3|26.589489067080606| 7.668522592115432|[2.0,3.0]| 25.68035641846656|
|-128353485|  3|  3|31.989149994214028|7.5491212159433845|[3.0,3.0]| 27.16633775333512|
|-128353222|  1|  1|28.025719800232352| 7.963141354806845|[1.0,1.0]|25.636305865382393|
|-128349146|  1|  4| 21.07936493484888| 7.441836835256949|[1.0,4.0]|  23.4734096927058|
|-128348482|  0|  2| 20.15261750242769| 7.784772142372692|[0.0,2.0]|23.429359139621635|
|-128344989|  2|  2|30.467669883864204| 7.754308395571972|[2.0,2.0]| 26.40132180935876|
|-128343728|  1|  1| 27.75372310889385| 8.029805259888121|[1.0,1.0]|25.636305865382393|
|-128338893|  2|  4|27.912820097659736|7.3226383507304025|[2.0,4.0]|24.959391027574362|
|-128336617|  2|  2|28.016630392213372| 7.967178037130402|[2.0,2.0]| 26.40132180935876|
+----------+---+---+------------------+------------------+---------+------------------+
only showing top 20 rows


scala> model.transform(train).show()
+----------+---+---+------------------+------------------+-----------+------------------+
|        id|   |   |             label|                  |   features|        prediction|
+----------+---+---+------------------+------------------+-----------+------------------+
|-128398173| 10| 27|   20.689088691544|5.0831419702521385|[10.0,27.0]|20.265037716002297|
|-128397829|  1|  3|24.065572096781725| 7.497539429075076|  [1.0,3.0]|24.194375083597997|
|-128397797|  1|  1| 26.97496663166592| 8.052270122623218|  [1.0,1.0]|25.636305865382393|
|-128397047|  0|  1|23.638990984594173| 8.106116057289787|  [0.0,1.0]|24.150324530513835|
|-128396750|  2|  3|26.630882351026454| 7.660736520814687|  [2.0,3.0]| 25.68035641846656|
|-128396552|  5|  7|27.736702156235467| 6.863038043612407|  [5.0,7.0]|27.254438859503452|
|-128395862|  0|  1|23.120203102063922|  8.02096736583166|  [0.0,1.0]|24.150324530513835|
|-128395592| 38| 59| 30.24520249380323|  3.69801583396422|[38.0,59.0]| 38.80162258377168|
|-128395541|  5| 11| 22.26825708994913| 6.428754519813465| [5.0,11.0]| 24.37057729593466|
|-128395390|  1|  1|28.334513967790194| 8.019769727806871|  [1.0,1.0]|25.636305865382393|
|-128395244|  1|  3|23.465269461896373| 7.612651710816195|  [1.0,3.0]|24.194375083597997|
|-128395197|  0|  5| 18.39246980228457| 7.365140373595669|  [0.0,5.0]|21.266462966945042|
|-128395082|  4|  4|32.202466398183304| 7.466894460617119|  [4.0,4.0]|27.931353697311486|
|-128395051|  0|  2| 21.32127820261808| 7.877511119478758|  [0.0,2.0]|23.429359139621635|
|-128394880|  1|  1|28.043870756278054|  7.97881206147458|  [1.0,1.0]|25.636305865382393|
|-128394278|  1|  1|28.228062271998144|7.9949827798162545|  [1.0,1.0]|25.636305865382393|
|-128393384|  2|  2|28.841544567657408|7.8666392457260095|  [2.0,2.0]| 26.40132180935876|
|-128393117|  0|  1| 23.05333232104851| 8.057701028697512|  [0.0,1.0]|24.150324530513835|
|-128392938|  1|  1|27.478545260286488| 7.962923344228324|  [1.0,1.0]|25.636305865382393|
|-128391577|  5| 13|20.851925696264416| 6.010443876522932| [5.0,13.0]|22.928646514150266|
+----------+---+---+------------------+------------------+-----------+------------------+
only showing top 20 rows

